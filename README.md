# MultiKOC: Multi-One-Class Classifier Based K-Means Clustering

## Honour Code
 I shall be honest in my efforts and will make my parents proud. 
 signed: Ayush Sharma, Rachit Jain.

## Motivation
The motivation for using MultiKOC (Multi-one-class classifier based on K-means) is that multiple subclusters can occur in the class cluster of one-class classification problems which classic one-class classifiers fail to handle as they do not see the negative samples or sub-data, especially in computational biology such as multiple tumors, protein folds, biometrics, MRI, etc. The MultiKOC handles this issue by clustering the positive samples using K means and then training a one-class classifier for each cluster.

## Problem Definition
In many real world problems with two-class data, one of the clusters consists of multiple subclusters. Examples include but are not limited to, breast cancer data, thyroid gland data, iris classification dataset, etc. But, simple one-class classification assumes that the data only consists of two pure compact clusters. This leads to insufficient and less accurate results. Hence, a more reliable method for classification is needed.

## Methodology
The MultiKOC uses a simple methodology. It first takes into account only the positive samples, disregarding the negative samples. Then it uses K-means clustering (other clustering algorithms can be used according to the dataset) to divide the positive samples into clusters. The number of clusters is not critical, as identifying 2 different clusters as 1 is more problematic than dividing one cluster into 2 clusters. After clustering, one-class classifiers (J48, Random Forest, Naive Bayes, SVM, etc.) are trained on each cluster. 
Now, for a test instance, all the one-class classifiers are used, and if at least one of them identifies it as positive, then it is considered as positive, else negative.

<!-- ### **Algorithm** -->
<!-- ![](https://i.imgur.com/1zPdjHp.png)
**<center>Fig 1. Illustration of proposed method.</center>** -->

## Results and Conclusion 
The model was tested on three different datasets: a syntactic dataset, an **iris plant type** dataset, and a **thyroid gland dataset**.  In each experiment of OC classifiers, all algorithms were trained following the **80-20** convention: separate the positive class into 80% for training and 20% for testing with all the negative examples. For the two-class classifiers, data were split into 80-20 for both positive and negative data. The experiments are performed 100 times each to get average performance.

The performance of MultiKOC was tested using four different classifiers: **J48**, **SVM**, **Naïve Bayes**, and **Random Forest** against the classical counterparts of these classifiers. The MultiKOC was also tested with k = 1,2,3,4,5,6 which represents the number of clusters generated by **K-means**. In the case of using J48, SVM, and Naïve Bayes as classifiers, MultiKOC outperforms the classical one-class classifier and gives equivalent performance while using the Random Forest classifier. The performance difference is more than 10% in favor of MultiKOC when using the SVM classifier.

In summary, the MultiKOC outperforms or is comparable to the classical methods in all the algorithms used. The results are summarised in the table below:

![](https://i.imgur.com/rdvFOnq.png)


Another experiment using different values of k showed that the performance of the MultiKOC algorithm, in general, does not depend on the number of clusters but in some cases, it was affected. 

The current results show that it is possible to build up a multi-one-class classifier with a combined clustering beforehand process based only on positive examples yielding a significant improvement over the one-class and similar results as the two-class.

## Discussion
The MultiKOC approach based on the one-class classification method discussed in this paper is based on partitioning the training data into different clusters and constructing the one-class model for each cluster. The MultiKOC approach performs similarly to that of the two-class version but it includes more interpretable classifiers which help in performing deep analysis and exploring the hidden structure of the data. 

Further research could proceed in several exciting directions
* The current framework is tested only for classification type data. It could also be tested for other data types. 
* Second, the concept used in this approach can also be applied to other types of classifiers.
* In this framework, after checking with all the classifiers, we assign positive or negative class to the data point. Instead, a voting mechanism based on the results of multi one-class classifiers can be devised which assigns the label to this data point.
